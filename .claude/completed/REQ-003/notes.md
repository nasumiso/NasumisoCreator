# REQ-003: CoreMLでの高速化 - 作業ログ

## 実装の気づき

**CoreMLExecutionProviderの導入**:
- ONNX Runtime 1.15.0+に標準で含まれているため、追加パッケージ不要
- `providers` 引数でプロバイダーを指定するだけで有効化可能
- フォールバック機能が自動的に働く（CoreML → CPU）

**実装のポイント**:
```python
providers = ['CoreMLExecutionProvider', 'CPUExecutionProvider']
self.session = ort.InferenceSession(model_path, providers=providers)
```

**プロバイダー確認方法**:
```python
available_providers = self.session.get_providers()
# → ['CoreMLExecutionProvider', 'CPUExecutionProvider']
```

**テスト結果の考察**:
- MacBook Air M1でCoreMLが正常に動作
- 1471ノード中1088ノード（約74%）がCoreMLで実行
- 残り26%のノードはCPUで実行（CoreML非対応オペレーション）
- すべての画像が正常に処理され、タグ生成も問題なし

**技術的な発見**:
- CoreMLは全オペレーションをサポートしているわけではない
- ハイブリッド実行（CoreML + CPU）が自動的に行われる
- ログに警告が出るが、これは正常な動作（非対応ノードがある旨の通知）

**処理速度のベンチマーク結果**:

実測値（15枚の画像で測定）:
- **CPU専用**: 16.05秒（平均1.070秒/枚）
- **CoreML有効**: 43.80秒（平均2.920秒/枚）
- **結果**: CoreMLの方が約2.7倍遅い

**CoreMLが遅くなった理由**:
1. CoreMLとCPU間の切り替えオーバーヘッド（74%がCoreML、26%がCPU実行）
2. 小規模バッチ処理（1枚ずつ）でCoreML初期化コストが毎回発生
3. WD14 Taggerは軽量モデルのため、CPU実行でも十分高速
4. ONNXモデルをCoreMLに変換する際のオーバーヘッド

**最終的な対応**:
- デフォルトをCPU実行に変更
- `--use-coreml`オプションでCoreML有効化可能にした
- CoreML実装は将来的な活用のために残置

---

## ふりかえり・感想

### 良かった点
- CoreMLの実装自体は成功し、正常に動作することを確認できた
- ベンチマークを実施したことで、実用性を客観的に評価できた
- 「高速化技術が常に速いわけではない」という重要な知見が得られた
- 実装を残しつつデフォルトを変更するという柔軟な対応ができた

### 学んだこと
- ハードウェアアクセラレータは万能ではない
  - 小規模処理ではオーバーヘッドが支配的になる
  - モデルの特性（軽量・重量）によって効果が異なる
  - ハイブリッド実行（複数プロバイダー混在）のコストは大きい

- ベンチマークの重要性
  - 「高速化」を実装する前に、まず現状を測定すべきだった
  - 実測なしの最適化は危険（推測だけでは判断できない）

- オプション設計の柔軟性
  - デフォルトを実用的な設定にしつつ、実験的機能をオプションで提供
  - 将来的な活用可能性を残せる

### 改善できた点
- 最初にベンチマークを取ってから実装すべきだった
- CoreML実装前に「小規模バッチ処理では効果がない可能性」を予測できたかもしれない

### 今後への活かし方
- 大規模モデル（Stable Diffusionなど）ではCoreMLの効果を再検証する価値あり
- バッチ処理（複数画像を一度に処理）での効果測定も検討
- 他のハードウェアアクセラレータ（GPUなど）でも同様の考慮が必要
